{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELO_ Merchant_category_Recommendaton.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "7N0wxs7WSsu_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#KAGGLE CHALLENGE: \n",
        "#Elo Merchant Category Recommendation\n",
        "\n",
        "https://www.kaggle.com/c/elo-merchant-category-recommendation"
      ]
    },
    {
      "metadata": {
        "id": "0ML6a6pB_f9N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import missingno.missingno as ms\n",
        "import datetime\n",
        "import gc\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(4590)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XzPOA4ZBYhfV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Getting the data:"
      ]
    },
    {
      "metadata": {
        "id": "UbP5yHIz_itg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile('./train.csv') and not os.path.isfile('./test.csv'):\n",
        "  # Install Kaggle API for download competition data\n",
        "  !pip3 install -q kaggle\n",
        "  # enter your Kaggle credentionals here\n",
        "  os.environ['KAGGLE_USERNAME']=\"anushayeruva\"\n",
        "  os.environ['KAGGLE_KEY']=\"d3d423c1fa21e5a856df867e6a305fc8\"\n",
        "  # If you are unable to download the competition dataset, check to see if you have \n",
        "  # accepted the user agreement on the competition website. \n",
        "  !kaggle competitions download -c elo-merchant-category-recommendation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tL-LxWs1_1pV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "csWXFOty__KC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Unzipping the files.\n",
        "!unzip test.csv.zip \n",
        "!unzip train.csv.zip\n",
        "!unzip historical_transactions.csv.zip\n",
        "!unzip merchants.csv.zip\n",
        "!unzip new_merchant_transactions.csv.zip\n",
        "!unzip test.csv.zip\n",
        "!unzip train.csv.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CC3fDufTAB0M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x9VY0fJ4AjdY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Removing the zip files.\n",
        "! rm *.zip\n",
        "!chmod 777 *.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q7HmGmYgA5wE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "df_hist_trans = pd.read_csv('historical_transactions.csv')\n",
        "df_new_merchant_trans = pd.read_csv('new_merchant_transactions.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "klvd3-3CA_IN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# userdefined function to reduce the memory.\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AdFecAXuBgeu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reducing the memory usage for historical_Transaction and new_merchant_transaction data.\n",
        "df_hist_trans = reduce_mem_usage(df_hist_trans)\n",
        "df_new_merchant_trans = reduce_mem_usage(df_new_merchant_trans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQlcPjAoYnd4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Imputing the missing values:"
      ]
    },
    {
      "metadata": {
        "id": "KL16N-vXBlrG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imputing the Nan's with mode.\n",
        "for df in [df_hist_trans,df_new_merchant_trans]:\n",
        "    df['category_2'].fillna(1.0,inplace=True)\n",
        "    df['category_3'].fillna('A',inplace=True)\n",
        "    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "krPz1hj0JBSp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Userdefined function to do aggregation on features  to extract new features.\n",
        "def get_new_columns(name,aggs):\n",
        "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MKpCO4p4JScu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loyalty points will be assigned only after 2 months of card usage . this is not give in the data so to get that we are  extacting year,month,day,time  they start using the card.\n",
        "for df in [df_hist_trans,df_new_merchant_trans]:\n",
        "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
        "    df['year'] = df['purchase_date'].dt.year\n",
        "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
        "    df['month'] = df['purchase_date'].dt.month\n",
        "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
        "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int) # purchse happened in weekday or weekend.\n",
        "    df['hour'] = df['purchase_date'].dt.hour  # time of the day\n",
        "    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})  # Transaction authorized or not.\n",
        "    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) # converting string to boolean.\n",
        "    #https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\n",
        "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30  # number of months they have been using the card.\n",
        "    df['month_diff'] += df['month_lag']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGvA-1qzYviY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Feature extraction:"
      ]
    },
    {
      "metadata": {
        "id": "y2EaRxBwJeMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# doingn the aggregation on extracted columns.\n",
        "aggs = {}\n",
        "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
        "    aggs[col] = ['nunique']\n",
        "\n",
        "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
        "aggs['installments'] = ['sum','max','min','mean','var']\n",
        "aggs['purchase_date'] = ['max','min']\n",
        "aggs['month_lag'] = ['max','min','mean','var']\n",
        "aggs['month_diff'] = ['mean']\n",
        "aggs['authorized_flag'] = ['sum', 'mean']\n",
        "aggs['weekend'] = ['sum', 'mean']\n",
        "aggs['category_1'] = ['sum', 'mean']\n",
        "aggs['card_id'] = ['size']\n",
        "\n",
        "for col in ['category_2','category_3']:\n",
        "    df_hist_trans[col+'_mean'] = df_hist_trans.groupby([col])['purchase_amount'].transform('mean')\n",
        "    aggs[col+'_mean'] = ['mean']    \n",
        "\n",
        "new_columns = get_new_columns('hist',aggs)\n",
        "df_hist_trans_group = df_hist_trans.groupby('card_id').agg(aggs)\n",
        "df_hist_trans_group.columns = new_columns\n",
        "df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
        "df_hist_trans_group['hist_purchase_date_diff'] = (df_hist_trans_group['hist_purchase_date_max'] - df_hist_trans_group['hist_purchase_date_min']).dt.days\n",
        "df_hist_trans_group['hist_purchase_date_average'] = df_hist_trans_group['hist_purchase_date_diff']/df_hist_trans_group['hist_card_id_size']\n",
        "df_hist_trans_group['hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['hist_purchase_date_max']).dt.days # Number of days they have been using the card.\n",
        "df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left') # merging the extrcted features into train and test.\n",
        "df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n",
        "del df_hist_trans_group;gc.collect() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5rdv-MZULqWz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Repeating the same operation on new_merchants_transaction data.\n",
        "aggs = {}\n",
        "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
        "    aggs[col] = ['nunique']\n",
        "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
        "aggs['installments'] = ['sum','max','min','mean','var']\n",
        "aggs['purchase_date'] = ['max','min']\n",
        "aggs['month_lag'] = ['max','min','mean','var']\n",
        "aggs['month_diff'] = ['mean']\n",
        "aggs['weekend'] = ['sum', 'mean']\n",
        "aggs['category_1'] = ['sum', 'mean']\n",
        "aggs['card_id'] = ['size']\n",
        "\n",
        "for col in ['category_2','category_3']:\n",
        "    df_new_merchant_trans[col+'_mean'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('mean')\n",
        "    aggs[col+'_mean'] = ['mean']\n",
        "    \n",
        "new_columns = get_new_columns('new_hist',aggs)\n",
        "df_hist_trans_group = df_new_merchant_trans.groupby('card_id').agg(aggs)\n",
        "df_hist_trans_group.columns = new_columns\n",
        "df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
        "df_hist_trans_group['new_hist_purchase_date_diff'] = (df_hist_trans_group['new_hist_purchase_date_max'] - df_hist_trans_group['new_hist_purchase_date_min']).dt.days\n",
        "df_hist_trans_group['new_hist_purchase_date_average'] = df_hist_trans_group['new_hist_purchase_date_diff']/df_hist_trans_group['new_hist_card_id_size']\n",
        "df_hist_trans_group['new_hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['new_hist_purchase_date_max']).dt.days\n",
        "df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n",
        "df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n",
        "del df_hist_trans_group;gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bikRQPrvRQ5g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "del df_hist_trans;gc.collect()\n",
        "del df_new_merchant_trans;gc.collect()\n",
        "df_train.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GCZJEjo3R6f5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Target has outliers so creating a new column-outliers,to impute with mean of the fearure's group it belongs to.\n",
        "df_train['outliers'] = 0\n",
        "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
        "df_train['outliers'].value_counts()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "inCkXx5QJETP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_train2=df_train1.loc[df_train1['outliers']==0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sk30XnwqJMR5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(df_train2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7wvegboQSZS7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Converting the features into datetime data type to perform the date functions.\n",
        "for df in [df_train,df_test]:\n",
        "df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
        "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
        "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
        "    df['month'] = df['first_active_month'].dt.month\n",
        "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
        "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
        "    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
        "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n",
        "                     'new_hist_purchase_date_min']:\n",
        "        df[f] = df[f].astype(np.int64) * 1e-9\n",
        "    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n",
        "    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
        "\n",
        "for f in ['feature_1','feature_2','feature_3']:\n",
        "    order_label = df_train.groupby([f])['outliers'].mean() # imputing outliers in target column with mean.\n",
        "    df_train[f] = df_train[f].map(order_label)\n",
        "    df_test[f] = df_test[f].map(order_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RPXvSSBkSqna",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Checking if the column already exist in the train data or not. If not, adding those columns.\n",
        "df_train2_columns = [c for c in df_train2.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
        "target = df_train2['target']\n",
        "del df_train2['target']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJ4mzC96XTCM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqrWQYYSZE51",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Extracting the feature importance by using LGB algorithm.:"
      ]
    },
    {
      "metadata": {
        "id": "LrJs4UotX4HP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialling the parameters to perform Stratifiedkfold. \n",
        "param = {'num_leaves': 31,\n",
        "         'min_data_in_leaf': 30, \n",
        "         'objective':'regression',\n",
        "         'max_depth': -1,\n",
        "         'learning_rate': 0.01,\n",
        "         \"min_child_samples\": 20,\n",
        "         \"boosting\": \"gbdt\",\n",
        "         \"feature_fraction\": 0.9,\n",
        "         \"bagging_freq\": 1,\n",
        "         \"bagging_fraction\": 0.9 ,\n",
        "         \"bagging_seed\": 11,\n",
        "         \"metric\": 'rmse',\n",
        "         \"lambda_l1\": 0.1,\n",
        "         \"verbosity\": -1,\n",
        "         \"nthread\": 4,\n",
        "         \"random_state\": 4590}\n",
        "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
        "oof = np.zeros(len(df_train2))\n",
        "predictions = np.zeros(len(df_test))\n",
        "feature_importance_df = pd.DataFrame()\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train2,df_train2['outliers'].values)):\n",
        "    print(\"fold {}\".format(fold_))\n",
        "    trn_data = lgb.Dataset(df_train2.iloc[trn_idx][df_train2_columns], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n",
        "    val_data = lgb.Dataset(df_train2.iloc[val_idx][df_train2_columns], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n",
        "\n",
        "    num_round = 10000\n",
        "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n",
        "    oof[val_idx] = clf.predict(df_train2.iloc[val_idx][df_train2_columns], num_iteration=clf.best_iteration)\n",
        "    \n",
        "    fold_importance_df = pd.DataFrame()\n",
        "    fold_importance_df[\"Feature\"] = df_train2_columns\n",
        "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
        "    fold_importance_df[\"fold\"] = fold_ + 1\n",
        "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
        "    \n",
        "    predictions += clf.predict(df_test[df_train2_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
        "\n",
        "np.sqrt(mean_squared_error(oof, target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R_0NYf6ZX8fH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Plotting the feature importance.\n",
        "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
        "        .groupby(\"Feature\")\n",
        "        .mean()\n",
        "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
        "\n",
        "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
        "\n",
        "plt.figure(figsize=(14,25))\n",
        "sns.barplot(x=\"importance\",\n",
        "            y=\"Feature\",\n",
        "            data=best_features.sort_values(by=\"importance\",\n",
        "                                           ascending=False))\n",
        "plt.title('LightGBM Features (avg over folds)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('lgbm_importances.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Glvup7ZXYOv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Building a model using Catboost algorithm."
      ]
    },
    {
      "metadata": {
        "id": "cEwGPRIn3SN-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZeqAHuwZ3QLn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " %%time\n",
        " from catboost import CatBoostRegressor\n",
        " folds =KFold(n_splits=5, shuffle=True, random_state=15)\n",
        " oof_cat = np.zeros(len(df_train2))\n",
        " predictions_cat = np.zeros(len(df_test))\n",
        "\n",
        " for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train2,df_train2['outliers'].values)):\n",
        "     print(\"fold n°{}\".format(fold_ + 1))\n",
        "     trn_data, trn_y = df_train2.iloc[trn_idx][df_train2_columns], target.iloc[trn_idx].values\n",
        "     val_data, val_y = df_train2.iloc[val_idx][df_train2_columns], target.iloc[val_idx].values\n",
        "     print(\"-\" * 10 + \"Catboost \" + str(fold_) + \"-\" * 10)\n",
        "     cb_model = CatBoostRegressor(iterations=3000, learning_rate=0.1, depth=8, l2_leaf_reg=20, bootstrap_type='Bernoulli',  eval_metric='RMSE', metric_period=50, od_type='Iter', od_wait=45, random_seed=17, allow_writing_files=False)\n",
        "     cb_model.fit(trn_data, trn_y, eval_set=(val_data, val_y), cat_features=[], use_best_model=True, verbose=100)\n",
        "    \n",
        "     oof_cat[val_idx] = cb_model.predict(val_data)\n",
        "     predictions_cat += cb_model.predict(df_test[df_train2_columns]) / folds.n_splits\n",
        "    \n",
        " np.save('oof_cat',oof_cat)\n",
        " np.save('predictions_cat', predictions_cat)\n",
        " np.sqrt(mean_squared_error(target.values, oof_cat))\n",
        " gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FQ1Am5S-ZBLg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        " %%time\n",
        "  from catboost import CatBoostRegressor\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
        "oof_cat = np.zeros(len(df_train2))\n",
        "predictions_cat = np.zeros(len(df_test))\n",
        "\n",
        "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train2,df_train2['outliers'].values)):\n",
        "  print(\"fold n°{}\".format(fold_ + 1))\n",
        "  trn_data, trn_y = df_train2.iloc[trn_idx][df_train2_columns], target.iloc[trn_idx].values\n",
        "  val_data, val_y = df_train2.iloc[val_idx][df_train2_columns], target.iloc[val_idx].values\n",
        "  print(\"-\" * 10 + \"Catboost \" + str(fold_) + \"-\" * 10)\n",
        "  cb_model = CatBoostRegressor(iterations=3000, learning_rate=0.1, depth=8, l2_leaf_reg=20, bootstrap_type='Bernoulli',  eval_metric='RMSE', metric_period=50, od_type='Iter', od_wait=45, random_seed=17, allow_writing_files=False)\n",
        "  cb_model.fit(trn_data, trn_y, eval_set=(val_data, val_y), cat_features=[], use_best_model=True, verbose=True)\n",
        "    \n",
        "  oof_cat[val_idx] = cb_model.predict(val_data)\n",
        "  predictions_cat += cb_model.predict(test[features]) / folds.n_splits\n",
        "    \n",
        "np.save('oof_cat', oof_cat)\n",
        "np.save('predictions_cat', predictions_cat)\n",
        "np.sqrt(mean_squared_error(target.values, oof_cat))\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Ub4HHhTZ-OM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating a DataFrame and converting it into a csv file.\n",
        "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
        "sub_df[\"target\"] = predictions\n",
        "sub_df.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J61I4gX-aBlB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# From google colab downloading the file into local disk.\n",
        "from google.colab import files\n",
        "files.download('submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2mrUYoXCaTJA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}